{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## German to English Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on tensorflow's Seq2Seq tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the dataset\n",
    "\n",
    "Data from http://www.manythings.org/anki/ and contains an english sentence followed by the German Translation.\n",
    "\n",
    "Processing the data includes:\n",
    "1. Adding a start and end token to each sentence\n",
    "2. Removing special characters\n",
    "3. Creating a word index and reverse word index (word -> id and id -> word)\n",
    "4. Pad each sentence to a maximum length\n",
    "5. Putting this into a format that tensorflow can read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = os.getcwd() + \"/data/deu-eng/deu.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "    \n",
    "    # Adding space between word and punctuation\n",
    "    w = re.sub(r\"([?.!,])\", r\" \\1\", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    \n",
    "    # replacing everything with space except(a-z, A-Z, \".\", \"!\", \",\")\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,]+\", \" \", w)\n",
    "    \n",
    "    # Removing all special characters\n",
    "    w = w.rstrip().strip()\n",
    "    \n",
    "    #Adding start/end tokens\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> test !ng sent ance ! <end>'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_sentence(\"TĘŠt!ng Sent&änče ^^!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(path, num_examples):\n",
    "    lines = open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "    \n",
    "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')] for l in lines[:num_examples]]\n",
    "    \n",
    "    return word_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<start> hi . <end>', '<start> hallo ! <end>'],\n",
       " ['<start> hi . <end>', '<start> gru gott ! <end>'],\n",
       " ['<start> run ! <end>', '<start> lauf ! <end>'],\n",
       " ['<start> wow ! <end>', '<start> potzdonner ! <end>'],\n",
       " ['<start> wow ! <end>', '<start> donnerwetter ! <end>'],\n",
       " ['<start> fire ! <end>', '<start> feuer ! <end>'],\n",
       " ['<start> help ! <end>', '<start> hilfe ! <end>'],\n",
       " ['<start> help ! <end>', '<start> zu hulf ! <end>'],\n",
       " ['<start> stop ! <end>', '<start> stopp ! <end>'],\n",
       " ['<start> wait ! <end>', '<start> warte ! <end>'],\n",
       " ['<start> go on . <end>', '<start> mach weiter . <end>'],\n",
       " ['<start> hello ! <end>', '<start> hallo ! <end>'],\n",
       " ['<start> i ran . <end>', '<start> ich rannte . <end>'],\n",
       " ['<start> i see . <end>', '<start> ich verstehe . <end>'],\n",
       " ['<start> i see . <end>', '<start> aha . <end>'],\n",
       " ['<start> i try . <end>', '<start> ich probiere es . <end>'],\n",
       " ['<start> i won ! <end>', '<start> ich hab gewonnen ! <end>'],\n",
       " ['<start> i won ! <end>', '<start> ich habe gewonnen ! <end>'],\n",
       " ['<start> smile . <end>', '<start> lacheln ! <end>'],\n",
       " ['<start> cheers ! <end>', '<start> zum wohl ! <end>']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_test = create_dataset(path_to_file, 20)\n",
    "preprocess_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class that creates a word -> index mapping\n",
    "class LanguageIndex():\n",
    "    def __init__(self, lang):\n",
    "        self.lang = lang\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.vocab = set()\n",
    "        \n",
    "        self.create_index()\n",
    "    \n",
    "    def create_index(self):\n",
    "        for phrase in self.lang:\n",
    "            self.vocab.update(phrase.split(' '))\n",
    "            \n",
    "        self.vocab = sorted(self.vocab)\n",
    "        \n",
    "        self.word2idx['<pad>'] = 0\n",
    "        for index, word in enumerate(self.vocab):\n",
    "            self.word2idx[word] = index + 1\n",
    "        \n",
    "        for word, index in self.word2idx.items():\n",
    "            self.idx2word[index] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)\n",
    "\n",
    "def load_dataset(path, num_examples):\n",
    "    pairs = create_dataset(path, num_examples)\n",
    "    \n",
    "    input_lang = LanguageIndex(deu for en, deu in pairs)\n",
    "    target_lang = LanguageIndex(en for en, deu in pairs)\n",
    "    \n",
    "    input_tensor = [[input_lang.word2idx[s] for s in deu.split(' ')] for en, deu in pairs]\n",
    "    target_tensor = [[target_lang.word2idx[s] for s in en.split(' ')] for en, deu in pairs]\n",
    "    \n",
    "    max_length_input, max_length_target = max_length(input_tensor), max_length(target_tensor)\n",
    "    \n",
    "    # Padding input and output tensor to max length\n",
    "    input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor,\n",
    "                                                                maxlen=max_length_input,\n",
    "                                                                padding='post')\n",
    "    target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor,\n",
    "                                                                 maxlen=max_length_target,\n",
    "                                                                 padding='post')\n",
    "    \n",
    "    return input_tensor, target_tensor, input_lang, target_lang, max_length_input, max_length_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using only 10,000 sentences out of the roughly 200,000 available sentences in the downloaded data. Splitting these into a train and test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = 10000\n",
    "input_tensor, target_tensor, input_lang, target_lang, max_length_input, max_length_target = load_dataset(path_to_file, num_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(input_tensor, target_tensor, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating tf.data Dataset\n",
    "\n",
    "buffer_size = len(X_train)\n",
    "batch_size = 64\n",
    "n_batch = buffer_size // batch_size\n",
    "embedding_dim = 128\n",
    "units = 512\n",
    "vocab_input_size = len(input_lang.word2idx)\n",
    "vocab_target_size = len(target_lang.word2idx)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(buffer_size)\n",
    "dataset = dataset.batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3501"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2176"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_target_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing Encoder and Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru(units):\n",
    "    return tf.keras.layers.GRU(units, \n",
    "                               return_sequences=True,\n",
    "                               return_state=True,\n",
    "                               recurrent_activation=\"sigmoid\",\n",
    "                               recurrent_initializer=\"glorot_uniform\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(self.enc_units)\n",
    "        \n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state=hidden)\n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(self.dec_units)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "        # used for attention\n",
    "        self.W1 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.W2 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, x, hidden, enc_output):\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        score = self.V(tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis)))\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        \n",
    "        context_vector = attention_weights * enc_output\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        \n",
    "        output, state = self.gru(x)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        x = self.fc(output)\n",
    "        \n",
    "        return x, state, attention_weights\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.dec_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_input_size, embedding_dim, units, batch_size)\n",
    "decoder = Decoder(vocab_target_size, embedding_dim, units, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = 1 - np.equal(real, 0)\n",
    "    loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating trianing checkpoints\n",
    "checkpoint_dir = os.getcwd() + \"/tf_logs/german_english_trans/training_checkpoints\"\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                encoder=encoder,\n",
    "                                decoder=decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Steps\n",
    "\n",
    "1. pass input through encoder to produce encoder output and encoder hidden state\n",
    "2. encoder output and hidden state is passed to decoder along with decoder input, i.e. the start token\n",
    "3. decoder returns predictions and the decoder hidden state\n",
    "4. decoder hidden state passed back into the model and predictions are used to calculate loss\n",
    "5. use teacher forcing to decide the next input to decoder.  (Teacher forcing is the technique where the target word is passed as the next input to the decoder)\n",
    "6. calculate gradients and apply it to optimizer and backprogagate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 \tBatch 0 \tLoss2.35349\n",
      "Epoch 1 \tBatch 100 \tLoss1.93987\n",
      "Epoch 1 \tLoss 2.0931\n",
      "Time taken for 1 epoch 266.66598987579346 sec\n",
      "\n",
      "Epoch 2 \tBatch 0 \tLoss1.76712\n",
      "Epoch 2 \tBatch 100 \tLoss1.77102\n",
      "Epoch 2 \tLoss 1.7657\n",
      "Time taken for 1 epoch 232.2601079940796 sec\n",
      "\n",
      "Epoch 3 \tBatch 0 \tLoss1.58285\n",
      "Epoch 3 \tBatch 100 \tLoss1.55807\n",
      "Epoch 3 \tLoss 1.5617\n",
      "Time taken for 1 epoch 190.01617288589478 sec\n",
      "\n",
      "Epoch 4 \tBatch 0 \tLoss1.40374\n",
      "Epoch 4 \tBatch 100 \tLoss1.56057\n",
      "Epoch 4 \tLoss 1.4000\n",
      "Time taken for 1 epoch 224.32567405700684 sec\n",
      "\n",
      "Epoch 5 \tBatch 0 \tLoss1.24784\n",
      "Epoch 5 \tBatch 100 \tLoss1.28348\n",
      "Epoch 5 \tLoss 1.2803\n",
      "Time taken for 1 epoch 190.1360628604889 sec\n",
      "\n",
      "Epoch 6 \tBatch 0 \tLoss1.17420\n",
      "Epoch 6 \tBatch 100 \tLoss1.26946\n",
      "Epoch 6 \tLoss 1.1864\n",
      "Time taken for 1 epoch 224.3213050365448 sec\n",
      "\n",
      "Epoch 7 \tBatch 0 \tLoss1.07787\n",
      "Epoch 7 \tBatch 100 \tLoss1.03178\n",
      "Epoch 7 \tLoss 1.1029\n",
      "Time taken for 1 epoch 186.15093088150024 sec\n",
      "\n",
      "Epoch 8 \tBatch 0 \tLoss1.02844\n",
      "Epoch 8 \tBatch 100 \tLoss1.06482\n",
      "Epoch 8 \tLoss 1.0238\n",
      "Time taken for 1 epoch 227.3938431739807 sec\n",
      "\n",
      "Epoch 9 \tBatch 0 \tLoss0.87527\n",
      "Epoch 9 \tBatch 100 \tLoss0.88562\n",
      "Epoch 9 \tLoss 0.9462\n",
      "Time taken for 1 epoch 186.5598759651184 sec\n",
      "\n",
      "Epoch 10 \tBatch 0 \tLoss0.87553\n",
      "Epoch 10 \tBatch 100 \tLoss0.94841\n",
      "Epoch 10 \tLoss 0.8694\n",
      "Time taken for 1 epoch 227.22999596595764 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    start = time.time()\n",
    "    hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for (batch, (inp, targ)) in enumerate(dataset):\n",
    "        loss = 0\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            enc_output, enc_hidden = encoder(inp, hidden)\n",
    "            \n",
    "            dec_hidden = enc_hidden\n",
    "            dec_input = tf.expand_dims([target_lang.word2idx['<start>']] * batch_size, 1)\n",
    "            \n",
    "            # feeding target as next input\n",
    "            for t in range(1, targ.shape[1]):\n",
    "                predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "                loss += loss_function(targ[:,t], predictions)\n",
    "                \n",
    "                # using teacher forcing\n",
    "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "                \n",
    "            batch_loss = (loss / int(targ.shape[1]))\n",
    "            total_loss += batch_loss\n",
    "            \n",
    "            variables = encoder.variables + decoder.variables\n",
    "            \n",
    "            gradients = tape.gradient(loss, variables)\n",
    "            optimizer.apply_gradients(zip(gradients, variables))\n",
    "            \n",
    "            if batch % 50 == 0:\n",
    "                print('Epoch {} \\tBatch {} \\tLoss{:.5f}'.format(epoch + 1, batch, batch_loss.numpy()))\n",
    "                \n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "            \n",
    "    print('Epoch {} \\tLoss {:.4f}'.format(epoch + 1, total_loss / n_batch))\n",
    "    print('Time taken for 1 epoch {:.1} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translations\n",
    "\n",
    "Let's use the Neural Net to actually make translations!  There is no teacher forcing here.  The input to the decoder at each time step is its previous predictions along with the hidden state and encoder output.  Of course, prediction will stop when the model his the end token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence, encoder, decoder, input_lang, target_lang, max_length_input, max_length_target):\n",
    "    attention_plot = np.zeros((max_length_target, max_length_input))\n",
    "    \n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    inputs = [input_lang.word2idx[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_length_input, padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    \n",
    "    result = ''\n",
    "    \n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    \n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "    \n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([target_lang.word2idx['<start>']], 0)\n",
    "    \n",
    "    for t in range(max_length_target):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
    "        \n",
    "        # storing attention weights\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "        \n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        \n",
    "        result += target_lang.idx2word[predicted_id] + ' '\n",
    "        \n",
    "        if target_lang.idx2word[predicted_id] == '<end>':\n",
    "            return result, sentence , attention_plot\n",
    "        \n",
    "        # feed predicted ID back into model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "        \n",
    "    return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "    fontdict = {'fontsize': 14}\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, encoder, decoder, input_lang, target_lang, max_length_input, max_length_target):\n",
    "    result, sentence, attention_plot = evaluate(sentence, encoder, decoder, input_lang, \n",
    "                                                target_lang, max_length_input, max_length_target)\n",
    "    \n",
    "    print('Input: {}'.format(sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "    \n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "#     plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.checkpointable.util.CheckpointLoadStatus at 0xb2e3f7828>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Restoring latest checkpoint\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> wie gehts ? <end>\n",
      "Predicted translation: do you recycle ? <end> \n"
     ]
    }
   ],
   "source": [
    "# How are you?\n",
    "translate(u'wie gehts?', encoder, decoder, input_lang, target_lang, max_length_input, max_length_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> ich mag dich <end>\n",
      "Predicted translation: i want to you . <end> \n"
     ]
    }
   ],
   "source": [
    "# I like you\n",
    "translate(u'Ich mag dich', encoder, decoder, input_lang, target_lang, max_length_input, max_length_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> ich bin hier <end>\n",
      "Predicted translation: i m in home . <end> \n"
     ]
    }
   ],
   "source": [
    "# I am here\n",
    "translate(u'Ich bin hier', encoder, decoder, input_lang, target_lang, max_length_input, max_length_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> wo sind die hunde ? <end>\n",
      "Predicted translation: what are you ? <end> \n"
     ]
    }
   ],
   "source": [
    "# where are the dogs?\n",
    "translate(u'wo sind die hunde?', encoder, decoder, input_lang, target_lang, max_length_input, max_length_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> mogen katzen hunde <end>\n",
      "Predicted translation: mary likes him . <end> \n"
     ]
    }
   ],
   "source": [
    "# do cats like dogs?\n",
    "translate(u'mogen Katzen hunde', encoder, decoder, input_lang, target_lang, max_length_input, max_length_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> hallo freund <end>\n",
      "Predicted translation: hello ! <end> \n"
     ]
    }
   ],
   "source": [
    "# Hello friend!\n",
    "translate(u'Hallo Freund', encoder, decoder, input_lang, target_lang, max_length_input, max_length_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too bad considering we only trained on 8000 sentences for 10 epochs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What did we accomplish?\n",
    "\n",
    "1. Loaded and processed a dataset containing 10000 sentences of English next to their German translation\n",
    "2. Built an RNN model which included an embedding, and GRU units to compose an encoder and decoder with an additional attention vector input to the decoder.\n",
    "3. Trained the model and used it to make predictions on simple German sentences.  The results are quite entertaining!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
